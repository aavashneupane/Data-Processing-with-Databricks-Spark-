{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e729afb-e012-43a2-abcd-ffb2ff970684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data processing on a big dataset with Databricks Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7063ec6-2762-4d68-9f8e-7f0673298d34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setting up Azure blob storage and databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9664187-84f4-43a3-8bd6-89703352397d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name='bigdataassignment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca2661a-65a3-4c30-b0bd-785b1d0c6ef0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_access_key = \"secret key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db800a7b-45c1-4e05-a689-2e8cf7f2b464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blob_container_name = \"bigdatassignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c32ab2-d56d-47b6-a5e8-da8b53441716",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = f'wasbs://{blob_container_name}@{storage_account_name}.blob.core.windows.net',\n",
    "  mount_point = f'/mnt/{blob_container_name}/',\n",
    "  extra_configs = {'fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net': storage_account_access_key}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0591f5-c118-44dd-b27e-c461b4eae59f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2015.parquet', name='green_taxi_2015.parquet', size=404556105, modificationTime=1727786438000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2015_csv/', name='green_taxi_2015_csv/', size=0, modificationTime=1728124366000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2016.parquet', name='green_taxi_2016.parquet', size=346731598, modificationTime=1727786342000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2017.parquet', name='green_taxi_2017.parquet', size=251504885, modificationTime=1727786161000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2018.parquet', name='green_taxi_2018.parquet', size=193874396, modificationTime=1727786055000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2019.parquet', name='green_taxi_2019.parquet', size=142163406, modificationTime=1727785966000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2020.parquet', name='green_taxi_2020.parquet', size=37076741, modificationTime=1727786100000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2021.parquet', name='green_taxi_2021.parquet', size=23479865, modificationTime=1727786118000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/green_taxi_2022.parquet', name='green_taxi_2022.parquet', size=20085692, modificationTime=1727786152000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/taxi_zone_lookup.csv', name='taxi_zone_lookup.csv', size=12322, modificationTime=1728119446000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2015.parquet', name='yellow_taxi_2015.parquet', size=2896588873, modificationTime=1727792043000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2016.parquet', name='yellow_taxi_2016.parquet', size=2616839324, modificationTime=1727791743000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2017.parquet', name='yellow_taxi_2017.parquet', size=2072273391, modificationTime=1727790701000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2018.parquet', name='yellow_taxi_2018.parquet', size=2098068908, modificationTime=1727790918000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2019.parquet', name='yellow_taxi_2019.parquet', size=1788371641, modificationTime=1727790386000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2020.parquet', name='yellow_taxi_2020.parquet', size=533759081, modificationTime=1727791590000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2021.parquet', name='yellow_taxi_2021.parquet', size=678942509, modificationTime=1727791972000),\n",
      " FileInfo(path='dbfs:/mnt/bigdatassignment/yellow_taxi_2022.parquet', name='yellow_taxi_2022.parquet', size=875304042, modificationTime=1727792136000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/bigdatassignment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e43879a-923c-4fd6-954c-ee3e04563e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load green taxi parquet files\n",
    "green_taxi_2015 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2015.parquet\")\n",
    "green_taxi_2016 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2016.parquet\")\n",
    "green_taxi_2017 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2017.parquet\")\n",
    "green_taxi_2018 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2018.parquet\")\n",
    "green_taxi_2019 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2019.parquet\")\n",
    "green_taxi_2020 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2020.parquet\")\n",
    "green_taxi_2021 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2021.parquet\")\n",
    "green_taxi_2022 = spark.read.parquet(\"/mnt/bigdatassignment/green_taxi_2022.parquet\")\n",
    "\n",
    "# Load yellow taxi parquet files\n",
    "yellow_taxi_2015 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2015.parquet\")\n",
    "yellow_taxi_2016 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2016.parquet\")\n",
    "yellow_taxi_2017 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2017.parquet\")\n",
    "yellow_taxi_2018 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2018.parquet\")\n",
    "yellow_taxi_2019 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2019.parquet\")\n",
    "yellow_taxi_2020 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2020.parquet\")\n",
    "yellow_taxi_2021 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2021.parquet\")\n",
    "yellow_taxi_2022 = spark.read.parquet(\"/mnt/bigdatassignment/yellow_taxi_2022.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e61c1e25-4002-46d8-80ab-c3f665c5894e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Writing files to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cced45-5838-470c-aa67-745d53801921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save green taxi parquet files to DBFS\n",
    "green_taxi_2015.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2015.parquet\")\n",
    "green_taxi_2016.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2016.parquet\")\n",
    "green_taxi_2017.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2017.parquet\")\n",
    "green_taxi_2018.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2018.parquet\")\n",
    "green_taxi_2019.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2019.parquet\")\n",
    "green_taxi_2020.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2020.parquet\")\n",
    "green_taxi_2021.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2021.parquet\")\n",
    "green_taxi_2022.write.format(\"parquet\").save(\"/dbfs/tmp/green_taxi_2022.parquet\")\n",
    "\n",
    "# Save yellow taxi parquet files to DBFS\n",
    "yellow_taxi_2015.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2015.parquet\")\n",
    "yellow_taxi_2016.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2016.parquet\")\n",
    "yellow_taxi_2017.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2017.parquet\")\n",
    "yellow_taxi_2018.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2018.parquet\")\n",
    "yellow_taxi_2019.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2019.parquet\")\n",
    "yellow_taxi_2020.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2020.parquet\")\n",
    "yellow_taxi_2021.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2021.parquet\")\n",
    "yellow_taxi_2022.write.format(\"parquet\").save(\"/dbfs/tmp/yellow_taxi_2022.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e0911f-35bc-4955-ae0f-f9c94ad4b8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/dbfs/tmp/combined_cleaned_taxi_data.parquet/', name='combined_cleaned_taxi_data.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/combined_taxi_data_with_location.parquet/', name='combined_taxi_data_with_location.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2015.parquet/', name='green_taxi_2015.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2015_csv/', name='green_taxi_2015_csv/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2016.parquet/', name='green_taxi_2016.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2017.parquet/', name='green_taxi_2017.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2018.parquet/', name='green_taxi_2018.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2019.parquet/', name='green_taxi_2019.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2020.parquet/', name='green_taxi_2020.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2021.parquet/', name='green_taxi_2021.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/green_taxi_2022.parquet/', name='green_taxi_2022.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/taxi_zone_lookup/', name='taxi_zone_lookup/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2015.parquet/', name='yellow_taxi_2015.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2016.parquet/', name='yellow_taxi_2016.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2017.parquet/', name='yellow_taxi_2017.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2018.parquet/', name='yellow_taxi_2018.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2019.parquet/', name='yellow_taxi_2019.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2020.parquet/', name='yellow_taxi_2020.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2021.parquet/', name='yellow_taxi_2021.parquet/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/dbfs/tmp/yellow_taxi_2022.parquet/', name='yellow_taxi_2022.parquet/', size=0, modificationTime=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify files in DBFS\n",
    "dbutils.fs.ls(\"/dbfs/tmp/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a476bed2-1a4b-443a-96f7-b0cf7442ed28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verifying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0d0e9c-9b99-4392-893b-e2435fe48d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows for Green Taxi: 66200401\n"
     ]
    }
   ],
   "source": [
    "# Load all green taxi Parquet files from DBFS\n",
    "green_taxi_df = spark.read.parquet(\"/dbfs/tmp/green_taxi_*.parquet\")\n",
    "green_taxi_count = green_taxi_df.count()\n",
    "print(f\"Total number of rows for Green Taxi: {green_taxi_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca947dfd-a6c0-4a7e-aa94-c0d8e156f618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows for Yellow Taxi: 663055251\n"
     ]
    }
   ],
   "source": [
    "# Load all yellow taxi Parquet files from DBFS\n",
    "yellow_taxi_df = spark.read.parquet(\"/dbfs/tmp/yellow_taxi_*.parquet\")\n",
    "yellow_taxi_count = yellow_taxi_df.count()\n",
    "print(f\"Total number of rows for Yellow Taxi: {yellow_taxi_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a168ef0b-26f4-4aa7-865e-56707c52db4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#load reference file\n",
    "taxi_zone_lookup = spark.read.csv(\"/mnt/bigdatassignment/taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec301aa-2f05-41ab-8419-5b7b5d60f6ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxi_zone_lookup.write.format(\"csv\").save(\"/dbfs/tmp/taxi_zone_lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aaa657f-c39e-42d9-9193-9420afa4bcc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Testing to analyze the file size if the data was in CSV format instead of parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1f0cea-a943-4ea3-a6cf-79f362570960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7b30fa-6f46-4d5e-a189-591a8e4fbda1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "green_taxi_2015_df = spark.read.parquet(\"/dbfs/tmp/green_taxi_2015.parquet\")\n",
    "# Convert the Parquet file to CSV and save it to DBFS (Databricks File System)\n",
    "green_taxi_2015_df.write.format(\"csv\").save(\"/dbfs/tmp/green_taxi_2015_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c83d0e-c3ee-4dae-959b-5d78e8c674dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List the CSV files\n",
    "display(dbutils.fs.ls(\"/dbfs/tmp/green_taxi_2015_csv/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb72c3d9-425a-4730-b0ef-63759eaca7c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "green_taxi_2015_df = spark.read.parquet(\"/dbfs/tmp/green_taxi_2015.parquet\")\n",
    "# Convert the Parquet file to CSV and save it to azure\n",
    "green_taxi_2015_df.write.format(\"csv\").save(\"/mnt/bigdatassignment/green_taxi_2015_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48babbfe-6efd-482a-9922-edf550376c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/dbfs/tmp/green_taxi_2015.parquet/</td><td>green_taxi_2015.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2015_csv/</td><td>green_taxi_2015_csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2016.parquet/</td><td>green_taxi_2016.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2017.parquet/</td><td>green_taxi_2017.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2018.parquet/</td><td>green_taxi_2018.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2019.parquet/</td><td>green_taxi_2019.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2020.parquet/</td><td>green_taxi_2020.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2021.parquet/</td><td>green_taxi_2021.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/green_taxi_2022.parquet/</td><td>green_taxi_2022.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/taxi_zone_lookup/</td><td>taxi_zone_lookup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2015.parquet/</td><td>yellow_taxi_2015.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2016.parquet/</td><td>yellow_taxi_2016.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2017.parquet/</td><td>yellow_taxi_2017.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2018.parquet/</td><td>yellow_taxi_2018.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2019.parquet/</td><td>yellow_taxi_2019.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2020.parquet/</td><td>yellow_taxi_2020.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2021.parquet/</td><td>yellow_taxi_2021.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/dbfs/tmp/yellow_taxi_2022.parquet/</td><td>yellow_taxi_2022.parquet/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/dbfs/tmp/green_taxi_2015.parquet/",
         "green_taxi_2015.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2015_csv/",
         "green_taxi_2015_csv/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2016.parquet/",
         "green_taxi_2016.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2017.parquet/",
         "green_taxi_2017.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2018.parquet/",
         "green_taxi_2018.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2019.parquet/",
         "green_taxi_2019.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2020.parquet/",
         "green_taxi_2020.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2021.parquet/",
         "green_taxi_2021.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/green_taxi_2022.parquet/",
         "green_taxi_2022.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/taxi_zone_lookup/",
         "taxi_zone_lookup/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2015.parquet/",
         "yellow_taxi_2015.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2016.parquet/",
         "yellow_taxi_2016.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2017.parquet/",
         "yellow_taxi_2017.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2018.parquet/",
         "yellow_taxi_2018.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2019.parquet/",
         "yellow_taxi_2019.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2020.parquet/",
         "yellow_taxi_2020.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2021.parquet/",
         "yellow_taxi_2021.parquet/",
         0,
         0
        ],
        [
         "dbfs:/dbfs/tmp/yellow_taxi_2022.parquet/",
         "yellow_taxi_2022.parquet/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/dbfs/tmp/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79e341ff-1f8d-4834-abc6-27443fc18925",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data cleaning and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7589e9-052a-49ad-8f10-045093aeda60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, expr\n",
    "\n",
    "# Load the taxi dataset \n",
    "yellow_taxi_df = spark.read.parquet(\"/dbfs/tmp/yellow_*.parquet\")\n",
    "\n",
    "green_taxi_df = spark.read.parquet(\"/dbfs/tmp/green_*.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fada36e-46f0-47f7-95a1-1b35f4e565c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# performing necessary data transformation\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "\n",
    "# Define the cleaning logic as a function to apply it to both datasets\n",
    "def clean_data1(df):\n",
    "    # Remove trips where drop-off is before pick-up\n",
    "    df = df.filter(col(\"lpep_dropoff_datetime\") > col(\"lpep_pickup_datetime\"))\n",
    "    \n",
    "    # Remove trips outside of the date range (2015-2022)\n",
    "    df = df.filter((col(\"lpep_pickup_datetime\") >= \"2015-01-01\") & \n",
    "                   (col(\"lpep_dropoff_datetime\") <= \"2022-12-31\"))\n",
    "    \n",
    "    # Calculate trip duration and speed\n",
    "    df = df.withColumn(\"duration_hours\", \n",
    "                       (unix_timestamp(col(\"lpep_dropoff_datetime\")) - unix_timestamp(col(\"lpep_pickup_datetime\"))) / 3600)\n",
    "    df = df.withColumn(\"speed_kmh\", col(\"trip_distance\") / col(\"duration_hours\"))\n",
    "    \n",
    "    # Remove trips with negative or zero speed and very high speeds (> 100 km/h)\n",
    "    df = df.filter((col(\"speed_kmh\") > 0) & (col(\"speed_kmh\") <= 100))\n",
    "    \n",
    "    # Remove trips with unrealistic duration (less than 1 minute or more than 3 hours)\n",
    "    df = df.filter((col(\"duration_hours\") > 1/60) & (col(\"duration_hours\") <= 3))\n",
    "    \n",
    "    # Remove trips with unrealistic distances (less than 0.1 km or more than 100 km)\n",
    "    df = df.filter((col(\"trip_distance\") > 0.1) & (col(\"trip_distance\") <= 100))\n",
    "    \n",
    "    # Remove trips with unrealistic passenger count (e.g., between 1 and 6)\n",
    "    df = df.filter((col(\"passenger_count\") > 0) & (col(\"passenger_count\") <= 6))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data2(df):\n",
    "    # Remove trips where drop-off is before pick-up\n",
    "    df = df.filter(col(\"tpep_dropoff_datetime\") > col(\"tpep_pickup_datetime\"))\n",
    "    \n",
    "    # Remove trips outside of the date range (2015-2022)\n",
    "    df = df.filter((col(\"tpep_pickup_datetime\") >= \"2015-01-01\") & \n",
    "                   (col(\"tpep_dropoff_datetime\") <= \"2022-12-31\"))\n",
    "    \n",
    "    # Calculate trip duration and speed\n",
    "    df = df.withColumn(\"duration_hours\", \n",
    "                       (unix_timestamp(col(\"tpep_dropoff_datetime\")) - unix_timestamp(col(\"tpep_pickup_datetime\"))) / 3600)\n",
    "    df = df.withColumn(\"speed_kmh\", col(\"trip_distance\") / col(\"duration_hours\"))\n",
    "    \n",
    "    # Remove trips with negative or zero speed and very high speeds (> 100 km/h)\n",
    "    df = df.filter((col(\"speed_kmh\") > 0) & (col(\"speed_kmh\") <= 100))\n",
    "    \n",
    "    # Remove trips with unrealistic duration (less than 1 minute or more than 3 hours)\n",
    "    df = df.filter((col(\"duration_hours\") > 1/60) & (col(\"duration_hours\") <= 3))\n",
    "    \n",
    "    # Remove trips with unrealistic distances (less than 0.1 km or more than 100 km)\n",
    "    df = df.filter((col(\"trip_distance\") > 0.1) & (col(\"trip_distance\") <= 100))\n",
    "    \n",
    "    # Remove trips with unrealistic passenger count (e.g., between 1 and 6)\n",
    "    df = df.filter((col(\"passenger_count\") > 0) & (col(\"passenger_count\") <= 6))\n",
    "    \n",
    "    return df    \n",
    "\n",
    "# Apply the cleaning logic to both datasets\n",
    "cleaned_yellow_taxi_df = clean_data2(yellow_taxi_df)\n",
    "cleaned_green_taxi_df = clean_data1(green_taxi_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d728a2a-480d-4e50-aacf-3e2a98add76b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- duration_hours: double (nullable = true)\n",
      " |-- speed_kmh: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- duration_hours: double (nullable = true)\n",
      " |-- speed_kmh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema of both DataFrames\n",
    "cleaned_yellow_taxi_df.printSchema()\n",
    "cleaned_green_taxi_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abdfdf5-7067-46fa-ab74-a798d27d54f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename lpep_pickup_datetime and lpep_dropoff_datetime in green_taxi_df\n",
    "cleaned_green_taxi_df = cleaned_green_taxi_df.withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "                                             .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "# Rename tpep_pickup_datetime and tpep_dropoff_datetime in yellow_taxi_df\n",
    "cleaned_yellow_taxi_df = cleaned_yellow_taxi_df.withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "                                               .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6a7910-1a02-4301-a986-ea1333043a96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine the cleaned datasets\n",
    "combined_taxi_df = cleaned_yellow_taxi_df.unionByName(cleaned_green_taxi_df, allowMissingColumns=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78dbce72-4864-4cdd-bdc8-27ce1eb6de45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:445)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:834)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:445)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:834)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the schema of the combined DataFrame\n",
    "combined_taxi_df.printSchema()\n",
    "\n",
    "# Show the first few rows of the combined DataFrame\n",
    "combined_taxi_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f09881-0591-4927-9eec-abba30a7f794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the location data\n",
    "location_df = spark.read.csv(\"/dbfs/tmp/taxi_zone_lookup\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema of the location data\n",
    "location_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea257108-c64c-4108-afe1-1763f12074aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform the join for pickup location using alias\n",
    "combined_with_pickup_location_df = combined_taxi_df.alias(\"taxi\") \\\n",
    "    .join(location_df.alias(\"pickup_location\"), \n",
    "          col(\"taxi.PULocationID\") == col(\"pickup_location.LocationID\"), \"left\") \\\n",
    "    .select(\"taxi.*\", \n",
    "            col(\"pickup_location.Borough\").alias(\"pickup_borough\"),\n",
    "            col(\"pickup_location.Zone\").alias(\"pickup_zone\"),\n",
    "            col(\"pickup_location.service_zone\").alias(\"pickup_service_zone\"))\n",
    "\n",
    "# Perform the join for dropoff location using alias\n",
    "combined_with_location_df = combined_with_pickup_location_df.alias(\"taxi\") \\\n",
    "    .join(location_df.alias(\"dropoff_location\"), \n",
    "          col(\"taxi.DOLocationID\") == col(\"dropoff_location.LocationID\"), \"left\") \\\n",
    "    .select(\"taxi.*\", \n",
    "            col(\"dropoff_location.Borough\").alias(\"dropoff_borough\"),\n",
    "            col(\"dropoff_location.Zone\").alias(\"dropoff_zone\"),\n",
    "            col(\"dropoff_location.service_zone\").alias(\"dropoff_service_zone\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987f0eee-c8bb-4872-a424-1fe47c14c7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- duration_hours: double (nullable = true)\n",
      " |-- speed_kmh: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- pickup_borough: string (nullable = true)\n",
      " |-- pickup_zone: string (nullable = true)\n",
      " |-- pickup_service_zone: string (nullable = true)\n",
      " |-- dropoff_borough: string (nullable = true)\n",
      " |-- dropoff_zone: string (nullable = true)\n",
      " |-- dropoff_service_zone: string (nullable = true)\n",
      "\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+------------------+---------+---------+--------------+--------------------+-------------------+---------------+--------------------+--------------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|      duration_hours|         speed_kmh|ehail_fee|trip_type|pickup_borough|         pickup_zone|pickup_service_zone|dropoff_borough|        dropoff_zone|dropoff_service_zone|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+------------------+---------+---------+--------------+--------------------+-------------------+---------------+--------------------+--------------------+\n",
      "|       2|2020-10-25 11:37:09|2020-10-25 11:45:35|            1.0|         1.88|       1.0|                 N|         230|         234|         1.0|        8.0|  0.0|    0.5|      2.26|         0.0|                  0.3|       13.56|                 2.5|       NULL| 0.14055555555555554|13.375494071146246|     NULL|     NULL|     Manhattan|Times Sq/Theatre ...|        Yellow Zone|      Manhattan|            Union Sq|         Yellow Zone|\n",
      "|       2|2020-10-25 11:49:06|2020-10-25 11:56:53|            1.0|         1.26|       1.0|                 N|         164|         113|         1.0|        7.0|  0.0|    0.5|      2.06|         0.0|                  0.3|       12.36|                 2.5|       NULL|  0.1297222222222222| 9.713062098501071|     NULL|     NULL|     Manhattan|       Midtown South|        Yellow Zone|      Manhattan|Greenwich Village...|         Yellow Zone|\n",
      "|       2|2020-10-25 11:09:16|2020-10-25 11:10:56|            1.0|         0.37|       1.0|                 N|         239|         239|         1.0|        3.5|  0.0|    0.5|       1.0|         0.0|                  0.3|         7.8|                 2.5|       NULL|0.027777777777777776|             13.32|     NULL|     NULL|     Manhattan|Upper West Side S...|        Yellow Zone|      Manhattan|Upper West Side S...|         Yellow Zone|\n",
      "|       2|2020-10-25 11:14:04|2020-10-25 11:20:37|            1.0|         1.21|       1.0|                 N|         239|         142|         2.0|        6.5|  0.0|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|       NULL| 0.10916666666666666|11.083969465648854|     NULL|     NULL|     Manhattan|Upper West Side S...|        Yellow Zone|      Manhattan| Lincoln Square East|         Yellow Zone|\n",
      "|       2|2020-10-25 11:49:55|2020-10-25 11:52:33|            1.0|         0.61|       1.0|                 N|         230|         186|         2.0|        4.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         7.3|                 2.5|       NULL| 0.04388888888888889| 13.89873417721519|     NULL|     NULL|     Manhattan|Times Sq/Theatre ...|        Yellow Zone|      Manhattan|Penn Station/Madi...|         Yellow Zone|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+--------------------+------------------+---------+---------+--------------+--------------------+-------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the schema of the combined DataFrame\n",
    "combined_with_location_df.printSchema()\n",
    "\n",
    "# Show some sample rows\n",
    "combined_with_location_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be7b7a11-5e7c-4f77-af2a-4ff9a49de7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Saving to parquet and table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea9d484-bc6a-4f29-a29b-606c86c5d1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export the combined DataFrame to Parquet format\n",
    "combined_with_location_df.write.format(\"parquet\").save(\"/dbfs/tmp/combined.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2fe997-663f-4462-a480-54c758541c8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving as a persistent table  \n",
    "combined_with_location_df.write.saveAsTable(\"combined_taxi_data_tables\")\n",
    "\n",
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM combined_taxi_data_table LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551fd8a1-4982-44db-a2f3-fdac4d3e2728",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double, duration_hours: double, speed_kmh: double, ehail_fee: double, trip_type: double, pickup_borough: string, pickup_zone: string, pickup_service_zone: string, dropoff_borough: string, dropoff_zone: string, dropoff_service_zone: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_with_location_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63aa4fe5-5547-48b6-9767-5193ccf45abe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f4d37e-c0ca-4d55-a77f-1c331a0390c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for querying in-memory\n",
    "combined_with_location_df.createOrReplaceTempView(\"combined_taxi_data_view\")\n",
    "\n",
    "# Query the view with SQL\n",
    "#spark.sql(\"SELECT * FROM combined_taxi_data_view LIMIT 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfd6e397-72be-482a-9c4d-8b1e9840d9cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PART 2 Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c997e3-fdf8-4c74-b28c-7cd2a0c017d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------+--------------------+-------------------+-------------------+------------------------+\n",
      "|year_month|total_trips|day_with_most_trips|hour_with_most_trips|avg_passenger_count|avg_amount_per_trip|avg_amount_per_passenger|\n",
      "+----------+-----------+-------------------+--------------------+-------------------+-------------------+------------------------+\n",
      "|   2015-01|      82319|             Monday|                  13|               1.63|              14.01|                   11.48|\n",
      "|   2015-01|      45095|           Saturday|                   4|                1.7|              16.58|                   13.37|\n",
      "|   2015-01|      54453|             Sunday|                  23|               1.65|              16.99|                   13.94|\n",
      "|   2015-01|      97571|            Tuesday|                  21|               1.65|              14.75|                   12.11|\n",
      "|   2015-01|      23715|             Sunday|                   7|               1.53|              17.01|                   14.29|\n",
      "|   2015-01|     155892|           Saturday|                  22|               1.74|              14.33|                   11.27|\n",
      "|   2015-01|      86517|          Wednesday|                  10|                1.6|              14.26|                   11.96|\n",
      "|   2015-01|     156407|             Friday|                  23|               1.72|              15.34|                   12.22|\n",
      "|   2015-01|     110019|             Friday|                  12|               1.62|              14.59|                   12.05|\n",
      "|   2015-01|     138722|           Saturday|                  17|                1.7|              13.38|                   10.66|\n",
      "|   2015-01|      16808|            Tuesday|                   1|                1.6|              15.74|                   13.02|\n",
      "|   2015-01|      39121|          Wednesday|                   0|               1.64|              15.78|                   12.98|\n",
      "|   2015-01|      77009|            Tuesday|                  14|               1.63|              14.29|                   11.76|\n",
      "|   2015-01|     115388|           Thursday|                  15|               1.66|              15.57|                   12.65|\n",
      "|   2015-01|      91547|             Sunday|                  16|               1.69|              15.06|                   12.05|\n",
      "|   2015-01|     153059|             Friday|                  21|               1.69|               14.6|                   11.73|\n",
      "|   2015-01|     110129|          Wednesday|                   8|               1.59|              13.84|                   11.72|\n",
      "|   2015-01|      78427|            Tuesday|                  15|               1.65|               14.2|                   11.63|\n",
      "|   2015-01|      91060|          Wednesday|                  15|               1.64|              14.89|                   12.24|\n",
      "|   2015-01|     105148|             Friday|                  11|               1.62|              14.41|                   11.94|\n",
      "+----------+-----------+-------------------+--------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DATE_FORMAT(pickup_datetime, 'yyyy-MM') AS year_month, COUNT(*) AS total_trips,DATE_FORMAT(pickup_datetime, 'EEEE') AS day_with_most_trips,HOUR(pickup_datetime) AS hour_with_most_trips,ROUND(AVG(passenger_count), 2) AS avg_passenger_count,ROUND(AVG(total_amount), 2) AS avg_amount_per_trip,ROUND(AVG(total_amount / NULLIF(passenger_count, 0)), 2) AS avg_amount_per_passenger FROM combined_taxi_data_view GROUP BY DATE_FORMAT(pickup_datetime, 'yyyy-MM'),DATE_FORMAT(pickup_datetime, 'EEEE'),HOUR(pickup_datetime) ORDER BY year_month\").show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1399e32e-76fd-4b26-87a2-5e9dff759d9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785fc388-4ede-4a18-8f75-6ea2b91d64f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT pickup_service_zone AS taxi_color, ROUND(AVG((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60), 2) AS avg_duration_minutes,ROUND(percentile_approx((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60, 0.5), 2) AS median_duration_minutes,ROUND(MIN((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60), 2) AS min_duration_minutes,ROUND(MAX((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60), 2) AS max_duration_minutes,ROUND(AVG(trip_distance), 2) AS avg_distance_km,ROUND(percentile_approx(trip_distance, 0.5), 2) AS median_distance_km,ROUND(MIN(trip_distance), 2) AS min_distance_km,ROUND(MAX(trip_distance) , 2) AS max_distance_km,ROUND(AVG(trip_distance / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 3600)), 2) AS avg_speed_kmh,ROUND(percentile_approx(trip_distance / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 3600), 0.5) , 2) AS median_speed_kmh,ROUND(MIN(trip_distance / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 3600)), 2) AS min_speed_kmh,ROUND(MAX(trip_distance / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 3600)), 2) AS max_speed_kmh FROM combined_taxi_data_view GROUP BY taxi_color;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "613c890a-3d61-41b3-a9a1-6a09e7db10fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed79050-a0be-4851-a30b-5a1e166fbaca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT pickup_service_zone,pickup_borough,dropoff_borough,DATE_FORMAT(pickup_datetime, 'yyyy-MM') AS month,DATE_FORMAT(pickup_datetime, 'EEEE') AS day_of_week,HOUR(pickup_datetime) AS hour,COUNT(*) AS total_trips,ROUND(AVG(trip_distance), 2) AS avg_distance_km,ROUND(AVG(total_amount), 2) AS avg_amount_per_trip,ROUND(SUM(total_amount), 2) AS total_amount_paid FROM combined_taxi_data_view GROUP BY pickup_service_zone, pickup_borough, dropoff_borough, DATE_FORMAT(pickup_datetime, 'yyyy-MM'), DATE_FORMAT(pickup_datetime, 'EEEE'), HOUR(pickup_datetime)ORDER BY pickup_service_zone, pickup_borough, dropoff_borough, month, day_of_week, hour;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed30afd6-4289-4bd7-94a1-af32c0221609",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23742056-8905-4774-bbb8-09a57596891e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT (COUNT(CASE WHEN tip_amount > 0 THEN 1 END) * 100.0 / COUNT(*)) AS percentage_trips_with_tips FROM combined_taxi_data_view;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "907ad073-4486-4468-ac57-443d82a36a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2c9696-1203-4bb2-99a9-10e21718590d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT (COUNT(CASE WHEN tip_amount >= 5 THEN 1 END) * 100.0 / COUNT(*)) AS percentage_trips_with_tips_at_least_5 FROM combined_taxi_data_view WHERE tip_amount > 0;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7cccd4-e33d-45ea-ab00-6488f0690c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71028705-fe3a-444e-b1b4-6348670d8f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT CASE WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 5 THEN 'Under 5 Mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 5 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 10 THEN 'From 5 mins to 10 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 10 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 20 THEN 'From 10 mins to 20 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 20 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 30 THEN 'From 20 mins to 30 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 30 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 60 THEN 'From 30 mins to 60 mins' ELSE 'At least 60 mins' END AS duration_bin, ROUND(AVG(trip_distance / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 3600)) , 2) AS avg_speed_kmh, ROUND(AVG(trip_distance / total_amount), 2) AS avg_distance_per_dollar FROM combined_taxi_data_view GROUP BY CASE WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 5 THEN 'Under 5 Mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 5 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 10 THEN 'From 5 mins to 10 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 10 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 20 THEN 'From 10 mins to 20 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 20 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 30 THEN 'From 20 mins to 30 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 30 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 60 THEN 'From 30 mins to 60 mins' ELSE 'At least 60 mins' END ORDER BY duration_bin;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25ec9dd-52b1-4e73-8bbd-62a766094e14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7d3f613-9d91-43f4-8b4d-d7e4b49ff00f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT CASE WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 5 THEN 'Under 5 Mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 5 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 10 THEN 'From 5 mins to 10 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 10 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 20 THEN 'From 10 mins to 20 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 20 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 30 THEN 'From 20 mins to 30 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 30 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 60 THEN 'From 30 mins to 60 mins' ELSE 'At least 60 mins' END AS duration_bin,ROUND(AVG(total_amount / ((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60)), 2) AS avg_fare_per_minute,ROUND(AVG(total_amount), 2) AS avg_total_fare FROM combined_taxi_data_view GROUP BY CASE WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 5 THEN 'Under 5 Mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 5 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 10 THEN 'From 5 mins to 10 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 10 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 20 THEN 'From 10 mins to 20 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 20 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 30 THEN 'From 20 mins to 30 mins' WHEN (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 >= 30 AND (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60 < 60 THEN 'From 30 mins to 60 mins' ELSE 'At least 60 mins' END ORDER BY avg_fare_per_minute DESC;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f7022f-3361-4759-ab7e-da4e749c5a44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To maximize income, taxi drivers should target longer trips, specifically those lasting at least 60 minutes. This duration bin offers the highest average distance per dollar at 0.36 km/$, indicating that these trips are the most cost-efficient. With a reasonable average speed of 14.01 km/h, these longer journeys optimize earnings by avoiding heavy traffic and reducing the frequency of short trips that typically incur higher operational costs. Additionally, longer trips tend to result in higher customer satisfaction, which can lead to better tips. Therefore, focusing on longer routes such as airport transfers or inter-city travels can be more lucrative and provide more stable earnings for drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627044c4-6ceb-4854-91a6-2b90631fba9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PART 3: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f2b7298-a357-4a18-b956-29fda0c53b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Building baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6950f592-0fb7-4c2f-8aea-a79db48e59f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 78.25110258177644\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, lit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Calculate the average total_amount for baseline model\n",
    "filtered_df = combined_with_location_d.sample(False, 0.5).filter(col(\"total_amount\").isNotNull())\n",
    "\n",
    "baseline_avg = filtered_df.filter(\"pickup_datetime < '2022-10-01'\") \\\n",
    "                          .agg(avg(\"total_amount\").alias(\"baseline_avg\")) \\\n",
    "                          .collect()[0][\"baseline_avg\"]\n",
    "\n",
    "# Add the baseline prediction column\n",
    "baseline_df = combined_with_location_d.withColumn(\"baseline_prediction\", lit(baseline_avg))\n",
    "\n",
    "# Calculate RMSE for the baseline model\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"baseline_prediction\", metricName=\"rmse\")\n",
    "baseline_rmse = evaluator.evaluate(baseline_df)\n",
    "\n",
    "print(\"Baseline RMSE:\", baseline_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a774bdd-1d6a-4ed1-bcb4-504b1359f068",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Preparing data for training and testing <br>\n",
    "Building two models Linear Regressor and Random Forest Regressor <br> Defining the pipelines<br>Training and evaluating each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce44dc80-f05a-43dd-b5a9-1c117c45e56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Index categorical columns\n",
    "pickup_indexer = StringIndexer(inputCol=\"pickup_borough\", outputCol=\"pickup_borough_index\")\n",
    "dropoff_indexer = StringIndexer(inputCol=\"dropoff_borough\", outputCol=\"dropoff_borough_index\")\n",
    "\n",
    "# Select features for model training (excluding fare_amount and tolls_amount)\n",
    "selected_columns = [\"passenger_count\", \"trip_distance\", \"pickup_borough\", \"dropoff_borough\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "\n",
    "# Handle Nulls and Prepare Data\n",
    "combined_with_location_df = combined_with_location_df.fillna({\n",
    "    \"passenger_count\": 1,  # Assuming default 1 passenger if null\n",
    "    \"trip_distance\": 0,    # Assuming 0 km if null\n",
    "    \"congestion_surcharge\": 0,  # Assuming no surcharge if null\n",
    "    \"airport_fee\": 0,      # Assuming no airport fee if null\n",
    "    \"pickup_borough\": 'Unknown',  # Default category if null\n",
    "    \"dropoff_borough\": 'Unknown'  # Default category if null\n",
    "})\n",
    "\n",
    "# Prepare the training data by filtering and sampling\n",
    "train_data = combined_with_location_df.filter(\"pickup_datetime < '2022-10-01'\") \\\n",
    "                                      .select(selected_columns + [\"total_amount\"]) \\\n",
    "                                      .sample(False, 0.5)  \n",
    "\n",
    "# Prepare the test data (October, November, December 2022)\n",
    "test_data = combined_with_location_df.filter(\"pickup_datetime >= '2022-10-01'\") \\\n",
    "                                     .select(selected_columns + [\"total_amount\"]).sample(False, 0.5)\n",
    "\n",
    "# Assemble numerical features along with the indexed categorical features into a single feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"passenger_count\", \"trip_distance\", \"congestion_surcharge\", \"airport_fee\", \n",
    "               \"pickup_borough_index\", \"dropoff_borough_index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Define a Linear Regression model on the 'total_amount'\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "\n",
    "# Define a Random Forest Regressor model\n",
    "rf = RandomForestRegressor(numTrees=2, maxDepth=1, featuresCol='features', labelCol='total_amount')\n",
    "\n",
    "# Pipelines\n",
    "lr_pipeline = Pipeline(stages=[pickup_indexer, dropoff_indexer, assembler, lr])\n",
    "rf_pipeline = Pipeline(stages=[pickup_indexer, dropoff_indexer, assembler, rf])\n",
    "\n",
    "# Run the pipeline for Linear Regression\n",
    "lr_model = lr_pipeline.fit(train_data)\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_rmse = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {lr_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a337a39c-ef46-4f96-b42d-beeb2913bcf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_model = rf_pipeline.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "rf_rmse = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest RMSE: {rf_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd56812f-0313-4eea-96c0-d03a40410e10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Choosing the best model (The model with the lowest RMSE will be chosen as the best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f633a8a-94fc-4757-bcec-e7318fcdc123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model = \"Linear Regression\" if lr_rmse < rf_rmse else \"Random Forest\"\n",
    "best_rmse = min(lr_rmse, rf_rmse)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model RMSE:\", best_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02921d61-cfe3-4b74-bcf2-6bbd41b8d64e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the comparison of these RMSE values, the model with the lowest error was identified as the best predictor. The Linear Regression model outperformed the Random Forest model and baseline model in this case, demonstrating its effectiveness for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "495173d8-90d6-4352-9be7-204e4cfd2cfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TLC",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
